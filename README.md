# ğŸ’ :Generations Capstone Project: NHL Game Data Pipeline â€” End-to-End Data Engineering in Microsoft Fabric 

This project demonstrates a **complete end-to-end data pipeline** built on **Microsoft Fabric**, using the *NHL Game Data* dataset from Kaggle.  
It covers the full data lifecycle â€” from raw data ingestion to analytics visualization â€” leveraging **Python**, **SQL**, **Azure SQL Database**, and **Power BI**.

---

## ğŸš€ Project Overview

**Goal:** Build an automated data pipeline that ingests, transforms, and visualizes NHL game statistics to uncover insights into team performance and player metrics.

**Pipeline Flow:**
1. **Ingest** â€” Download raw data from Kaggle using `kagglehub`  
2. **Transform** â€” Clean and shape data using Python (`pandas`)  
3. **Load** â€” Push curated tables into **Azure SQL Database** within Microsoft Fabric  
4. **Visualize** â€” Connect **Power BI** to Fabric for dynamic dashboards

---

## ğŸ§± Tech Stack

| Component | Technology |
|------------|-------------|
| **Extraction / Ingestion** | Python (`kagglehub`, `pandas`) |
| **Transformation** | Python, SQL |
| **Storage** | Azure SQL Database (Fabric-integrated) |
| **Orchestration** | Microsoft Fabric Data Pipeline |
| **Visualization** | Power BI (Fabric workspace) |

---

## ğŸ“‚ Repository Structure
<pre>
  
ğŸ“¦ nhl-data-pipeline/
â”‚
â”œâ”€â”€ data/ # Datasets (anonymized or sample)
â”‚ â”œâ”€â”€ raw/ # Original Kaggle dataset
â”‚ â”œâ”€â”€ processed/ # Cleaned dataset ready for modeling
â”‚ â””â”€â”€ data_dictionary.md
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingest/ # Ingestion scripts
â”‚ â”‚ â””â”€â”€ ingest_data.py
â”‚ â”œâ”€â”€ transform/ # Data cleaning, joins, validation
â”‚ â”‚ â””â”€â”€ transform_data.py
â”‚ â”œâ”€â”€ load/ # Load data into Azure SQL / Fabric
â”‚ â”‚ â””â”€â”€ load_to_powerbi.py
â”‚ â”œâ”€â”€ utils/ # Helper functions
â”‚ â”‚ â””â”€â”€ helpers.py
â”‚ â””â”€â”€ pipeline.py # Main orchestration entrypoint
â”‚
â”œâ”€â”€ sql/ # SQL queries for transformation
â”‚ â”œâ”€â”€ staging_queries.sql
â”‚ â”œâ”€â”€ transformation_queries.sql
â”‚ â””â”€â”€ data_validation.sql
â”‚
â”œâ”€â”€ powerbi/ # Dashboards and visual reports
â”‚ â”œâ”€â”€ dashboard_preview.png
â”‚ â””â”€â”€ report_description.md
â”‚
â”œâ”€â”€ docs/ # Architecture & documentation
â”‚ â”œâ”€â”€ architecture_diagram.png
â”‚ â”œâ”€â”€ pipeline_flow.md
â”‚ â”œâ”€â”€ setup_instructions.md
â”‚ â””â”€â”€ learnings.md
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

</pre>
---


## ğŸ”§ Tech Stack
- **Languages:** Python, SQL  
- **Tools:** Microsoft Fabric, Power BI, Azure Data Lake  
- **Libraries:** pandas, pyodbc, requests, etc.

## â–¶ï¸ How to Run
1. Clone the repo  
2. Set up your environment using `requirements.txt`  
3. Update your credentials in `config/connections.yaml`  
4. Run `python src/pipeline.py`

## ğŸ§  Learnings
- Hands-on with Microsoft Fabric data pipelines  
- Best practices for modular ETL design  


---

## âš™ï¸ How to Run the Pipeline

### 1ï¸âƒ£ Setup Environment
Install dependencies locally or in Microsoft Fabric Notebook:
```bash
pip install -r requirements.txt
- Data validation and automation scripting

